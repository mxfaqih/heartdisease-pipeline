{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wE7KjiMGWSXy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from typing import Text\n",
        "\n",
        "from absl import logging\n",
        "from tfx.orchestration import metadata, pipeline\n",
        "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
        "from modules import components\n",
        "\n",
        "PIPELINE_NAME = \"heartdisease-pipeline\"\n",
        "\n",
        "# pipeline inputs\n",
        "DATA_ROOT = \"data\"\n",
        "TRANSFORM_MODULE_FILE = \"modules/heartdisease_transform.py\"\n",
        "TRAINER_MODULE_FILE = \"modules/heartdisease_trainer.py\"\n",
        "TUNER_MODULE_FILE = 'modules/heartdisease_tuner.py'\n",
        "# requirement_file = os.path.join(root, \"requirements.txt\")\n",
        "\n",
        "\n",
        "OUTPUT_BASE = \"output\"\n",
        "serving_model_dir = os.path.join(OUTPUT_BASE, 'serving_model')\n",
        "pipeline_root = os.path.join(OUTPUT_BASE, PIPELINE_NAME)\n",
        "metadata_path = os.path.join(pipeline_root, \"metadata.sqlite\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def init_local_pipeline(\n",
        "    components, pipeline_root: Text\n",
        ") -> pipeline.Pipeline:\n",
        "    \"\"\"Init local pipeline\n",
        "\n",
        "    Args:\n",
        "        components (dict): tfx components\n",
        "        pipeline_root (Text): path to pipeline directory\n",
        "\n",
        "    Returns:\n",
        "        pipeline.Pipeline: apache beam pipeline orchestration\n",
        "    \"\"\"\n",
        "    logging.info(f\"Pipeline root set to: {pipeline_root}\")\n",
        "    beam_args = beam_args = [\n",
        "    '--direct_running_mode=multi_processing',\n",
        "    '--direct_num_workers=0'\n",
        "]\n",
        "\n",
        "    return pipeline.Pipeline(\n",
        "        pipeline_name=PIPELINE_NAME,\n",
        "        pipeline_root=pipeline_root,\n",
        "        components=components,\n",
        "        enable_cache=True,\n",
        "        metadata_connection_config=metadata.sqlite_metadata_connection_config(\n",
        "            metadata_path\n",
        "        ),\n",
        "        beam_pipeline_args=beam_args\n",
        "    )\n",
        ""
      ],
      "metadata": {
        "id": "99d02WroWkdm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'data_dir': DATA_ROOT,\n",
        "    'transform_module': TRANSFORM_MODULE_FILE,\n",
        "    'tuner_module': TUNER_MODULE_FILE,\n",
        "    'training_module': TRAINER_MODULE_FILE,\n",
        "    'training_steps': 5000,\n",
        "    'eval_steps': 1000,\n",
        "    'serving_model_dir': serving_model_dir\n",
        "}\n",
        "\n",
        "pipeline_components = components.init_components(config)\n",
        "\n",
        "\n",
        "pipeline = init_local_pipeline(pipeline_components, pipeline_root)\n",
        "BeamDagRunner().run(pipeline=pipeline)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pSvdAwo9Wl5f",
        "outputId": "220f439a-2bea-4920-f5a8-e70afb40a1d9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
              "          var jqueryScript = document.createElement('script');\n",
              "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
              "          jqueryScript.type = 'text/javascript';\n",
              "          jqueryScript.onload = function() {\n",
              "            var datatableScript = document.createElement('script');\n",
              "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
              "            datatableScript.type = 'text/javascript';\n",
              "            datatableScript.onload = function() {\n",
              "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
              "              window.interactive_beam_jquery(document).ready(function($){\n",
              "                \n",
              "              });\n",
              "            }\n",
              "            document.head.appendChild(datatableScript);\n",
              "          };\n",
              "          document.head.appendChild(jqueryScript);\n",
              "        } else {\n",
              "          window.interactive_beam_jquery(document).ready(function($){\n",
              "            \n",
              "          });\n",
              "        }"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
            "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
            "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/data/experimental/ops/readers.py:1086: parse_example_dataset (from tensorflow.python.data.experimental.ops.parsing_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(tf.io.parse_example(...))` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " gender_xf (InputLayer)      [(None, 3)]                  0         []                            \n",
            "                                                                                                  \n",
            " age_xf (InputLayer)         [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " impluse_xf (InputLayer)     [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " pressurehight_xf (InputLay  [(None, 1)]                  0         []                            \n",
            " er)                                                                                              \n",
            "                                                                                                  \n",
            " pressurelow_xf (InputLayer  [(None, 1)]                  0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " glucose_xf (InputLayer)     [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " kcm_xf (InputLayer)         [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " troponin_xf (InputLayer)    [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 10)                   0         ['gender_xf[0][0]',           \n",
            "                                                                     'age_xf[0][0]',              \n",
            "                                                                     'impluse_xf[0][0]',          \n",
            "                                                                     'pressurehight_xf[0][0]',    \n",
            "                                                                     'pressurelow_xf[0][0]',      \n",
            "                                                                     'glucose_xf[0][0]',          \n",
            "                                                                     'kcm_xf[0][0]',              \n",
            "                                                                     'troponin_xf[0][0]']         \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 16)                   176       ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 16)                   272       ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 16)                   272       ['dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 16)                   0         ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 1)                    17        ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 737 (2.88 KB)\n",
            "Trainable params: 737 (2.88 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/3\n",
            "4996/5000 [============================>.] - ETA: 0s - loss: 0.3851 - binary_accuracy: 0.8235\n",
            "Epoch 1: val_binary_accuracy improved from -inf to 0.89019, saving model to output/heartdisease-pipeline/Trainer/model/16/Format-Serving\n",
            "5000/5000 [==============================] - 40s 7ms/step - loss: 0.3850 - binary_accuracy: 0.8236 - val_loss: 0.2875 - val_binary_accuracy: 0.8902\n",
            "Epoch 2/3\n",
            "4996/5000 [============================>.] - ETA: 0s - loss: 0.2076 - binary_accuracy: 0.9208\n",
            "Epoch 2: val_binary_accuracy improved from 0.89019 to 0.94117, saving model to output/heartdisease-pipeline/Trainer/model/16/Format-Serving\n",
            "5000/5000 [==============================] - 35s 7ms/step - loss: 0.2076 - binary_accuracy: 0.9208 - val_loss: 0.1953 - val_binary_accuracy: 0.9412\n",
            "Epoch 3/3\n",
            "4994/5000 [============================>.] - ETA: 0s - loss: 0.1399 - binary_accuracy: 0.9474\n",
            "Epoch 3: val_binary_accuracy did not improve from 0.94117\n",
            "5000/5000 [==============================] - 34s 7ms/step - loss: 0.1398 - binary_accuracy: 0.9475 - val_loss: 0.2414 - val_binary_accuracy: 0.9373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1714525603   nanos: 391425371 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/worker/sdk_worker_main.py:361\" thread: \"MainThread\" \n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1714525603   nanos: 399352073 } message: \"Discarding unparseable args: [\\'--direct_runner_use_stacked_bundle\\', \\'--pipeline_type_check\\']\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/options/pipeline_options.py:372\" thread: \"MainThread\" \n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1714525603   nanos: 445846796 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/worker/sdk_worker_main.py:361\" thread: \"MainThread\" \n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1714525603   nanos: 453304290 } message: \"Discarding unparseable args: [\\'--direct_runner_use_stacked_bundle\\', \\'--pipeline_type_check\\']\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/options/pipeline_options.py:372\" thread: \"MainThread\" \n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1714525612   nanos: 413878202 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_27\" transform_id: \"ReadFromTFRecordToArrow[eval][0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/io/tfrecordio.py:59\" thread: \"Thread-10\" \n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1714525612   nanos: 773518800 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_28\" transform_id: \"ReadFromTFRecordToArrow[eval][0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/io/tfrecordio.py:59\" thread: \"Thread-10\" \n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow_model_analysis/writers/metrics_plots_and_validations_writer.py:112: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}